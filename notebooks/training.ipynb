{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnikolamilicevic\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/nucl-protein/notebooks/wandb/run-20250311_174148-5bv5gykj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nikolamilicevic/nuclprot/runs/5bv5gykj' target=\"_blank\">testing nuclprot crossattn</a></strong> to <a href='https://wandb.ai/nikolamilicevic/nuclprot' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nikolamilicevic/nuclprot' target=\"_blank\">https://wandb.ai/nikolamilicevic/nuclprot</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nikolamilicevic/nuclprot/runs/5bv5gykj' target=\"_blank\">https://wandb.ai/nikolamilicevic/nuclprot/runs/5bv5gykj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "WARMUP_STEPS = 5000\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 64\n",
    "LR = 2e-4\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "wandb.init(\n",
    "    project=\"nuclprot\",\n",
    "    name=\"testing nuclprot crossattn\",\n",
    "    config={\n",
    "        \"WARMUP_STEPS\": WARMUP_STEPS,\n",
    "        \"EPOCHS\": EPOCHS,\n",
    "        \"BATCH_SIZE\": BATCH_SIZE,\n",
    "        \"LR\": LR,\n",
    "    },\n",
    ")\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"vladak/anthem_hla_seq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['HLA', 'peptide', 'Label', 'Length', 'Sequence'],\n",
       "        num_rows: 539019\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['HLA', 'peptide', 'Label', 'Length', 'Sequence'],\n",
       "        num_rows: 172580\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class EmbeddingCache(torch.utils.data.Dataset):\n",
    "    \"\"\"This class will precompute embeddings for the data and cache\n",
    "    them for future reuse.\"\"\"\n",
    "    def __init__(self, data, key, value, emb_model, device) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: Huggingface dataset that will be cached.\n",
    "            key: Column with unique values for each sample (e.g. raw sequence).\n",
    "            value: The value that we are computing embedding for and caching.\n",
    "            emb_model: Model used for computing embeddings.\n",
    "            device: Device on which embeddings will reside.\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.data = self._filter_duplicates(data, key)\n",
    "        self.cache = self._cache_embeddings(data, value)\n",
    "        \n",
    "        \n",
    "        pass\n",
    "\n",
    "    def _filter_duplicates(self, data, key):\n",
    "        seen = set()\n",
    "        filtered_dataset =  data['train'].filter(lambda example: not (example[key] in seen or seen.add(example[key])))\n",
    "        return filtered_dataset\n",
    "    \n",
    "    def _cache_embeddings(self, data, value):\n",
    "        # ret cache\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e608da4f70884e9782d0e38c39acac4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/539019 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['HLA', 'peptide', 'Label', 'Length', 'Sequence'],\n",
       "    num_rows: 112\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_seq = ds['train'].unique(\"Sequence\")\n",
    "seen = set()\n",
    "filtered_dataset =  ds['train'].filter(lambda example: not (example[\"Sequence\"] in seen or seen.add(example[\"Sequence\"])))\n",
    "filtered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['HLA', 'peptide', 'Label', 'Length', 'Sequence'],\n",
       "        num_rows: 26950\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['HLA', 'peptide', 'Label', 'Length', 'Sequence'],\n",
       "        num_rows: 8629\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subsample\n",
    "from datasets import DatasetDict\n",
    "ds = DatasetDict({\n",
    "    split: ds[split].shuffle(seed=42).select(range(int(0.05 * len(ds[split]))))\n",
    "    for split in ds\n",
    "})\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d5ca4cd9f6426c92f18ba7474007d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26950 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb58b2dcf4784165ac58855512e86c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8629 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import hashlib\n",
    "def get_sequence_id(example):\n",
    "    example['seqhash'] = int(hashlib.sha256(example['Sequence'].encode()).hexdigest(), 16) % (10**9)\n",
    "    return example\n",
    "\n",
    "ds = ds.map(get_sequence_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer, DataCollatorWithPadding\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CrossAttentionLayer(nn.Module):\n",
    "    def __init__(self, embed_dim=512, num_heads=8, dropout=0.1):\n",
    "        super(CrossAttentionLayer, self).__init__()\n",
    "        self.modality1_to_modality2_attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.modality2_to_modality1_attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        ffn_hidden_dim = embed_dim * 3\n",
    "        self.ffn_modality1 = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ffn_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ffn_hidden_dim, embed_dim),\n",
    "        )\n",
    "        self.ffn_modality2 = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ffn_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ffn_hidden_dim, embed_dim),\n",
    "        )\n",
    "\n",
    "        self.modality1_norm = nn.LayerNorm(embed_dim)\n",
    "        self.modality2_norm = nn.LayerNorm(embed_dim)\n",
    "        self.ffn_modality1_norm = nn.LayerNorm(embed_dim)\n",
    "        self.ffn_modality2_norm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, modality1_embedding, modality2_embedding, key_pad_mask_modality1, key_pad_mask_modality2):\n",
    "        # Modality1 attending to Modality2\n",
    "        attended_modality1, _ = self.modality1_to_modality2_attention(\n",
    "            query=modality1_embedding, \n",
    "            key=modality2_embedding,\n",
    "            value=modality2_embedding,\n",
    "            key_padding_mask=key_pad_mask_modality2\n",
    "        )\n",
    "        attended_modality1 = self.modality1_norm(modality1_embedding + attended_modality1)\n",
    "        x_modality1 = self.ffn_modality1(attended_modality1)\n",
    "        x_modality1 = self.ffn_modality1_norm(attended_modality1 + self.dropout(x_modality1))\n",
    "\n",
    "        # Modality2 attending to Modality1\n",
    "        attended_modality2, _ = self.modality2_to_modality1_attention(\n",
    "            query=modality2_embedding, \n",
    "            key=modality1_embedding, \n",
    "            value=modality1_embedding,\n",
    "            key_padding_mask=key_pad_mask_modality1\n",
    "        )\n",
    "        attended_modality2 = self.modality2_norm(modality2_embedding + attended_modality2)\n",
    "        x_modality2 = self.ffn_modality2(attended_modality2)\n",
    "        x_modality2 = self.ffn_modality2_norm(attended_modality2 + self.dropout(x_modality2))\n",
    "\n",
    "        return x_modality1, x_modality2\n",
    "\n",
    "# MultiHeadCrossAttention  \n",
    "# class CrossAttentionLayer(nn.Module):\n",
    "#     def __init__(self, embed_dim=512, num_heads=8, dropout=0.1, ffn_hidden_dim=2048):\n",
    "#         super(CrossAttentionLayer, self).__init__()\n",
    "#         self.protein_to_ligand_attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "#         self.ligand_to_protein_attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "#         ffn_hidden_dim = embed_dim * 3\n",
    "#         self.ffn_protein = nn.Sequential(\n",
    "#             nn.Linear(embed_dim, ffn_hidden_dim),\n",
    "#             nn.ReLU(),  # Non-linear activation\n",
    "#             nn.Linear(ffn_hidden_dim, embed_dim),\n",
    "#         )\n",
    "#         self.ffn_ligand = nn.Sequential(\n",
    "#             nn.Linear(embed_dim, ffn_hidden_dim),\n",
    "#             nn.ReLU(),  # Non-linear activation\n",
    "#             nn.Linear(ffn_hidden_dim, embed_dim),\n",
    "#         )\n",
    "#         self.protein_norm = nn.LayerNorm(embed_dim)\n",
    "#         self.ligand_norm = nn.LayerNorm(embed_dim)\n",
    "#         self.ffn_protein_norm = nn.LayerNorm(embed_dim)\n",
    "#         self.ffn_ligand_norm = nn.LayerNorm(embed_dim)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, protein_embedding, ligand_embedding, key_pad_mask_prot, key_pad_mask_ligand):\n",
    "#         # Protein attending to ligand\n",
    "#         attended_protein, _ = self.protein_to_ligand_attention(\n",
    "#             query=protein_embedding, \n",
    "#             key=ligand_embedding,\n",
    "#             value=ligand_embedding,\n",
    "#             key_padding_mask=key_pad_mask_ligand\n",
    "#         )\n",
    "#         attended_protein = self.protein_norm(protein_embedding + attended_protein)  # Residual connection\n",
    "#         x_prot = self.ffn_protein(attended_protein)\n",
    "#         x_prot = self.ffn_protein_norm(attended_protein + self.dropout(x_prot))\n",
    "\n",
    "#         # Ligand attending to protein\n",
    "#         attended_ligand, _ = self.ligand_to_protein_attention(\n",
    "#             query=ligand_embedding, \n",
    "#             key=protein_embedding, \n",
    "#             value=protein_embedding,\n",
    "#             key_padding_mask=key_pad_mask_prot\n",
    "#         )\n",
    "#         attended_ligand = self.ligand_norm(ligand_embedding + attended_ligand)  # Residual connection\n",
    "#         x_ligand = self.ffn_ligand(attended_ligand)\n",
    "#         x_ligand = self.ffn_ligand_norm(attended_ligand + self.dropout(x_ligand))\n",
    "#         return x_prot, x_ligand\n",
    "\n",
    "class BindingAffinityModelWithMultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self, modality1_model_name, modality2_model_name, num_layers=3, hidden_dim=1024):\n",
    "        super().__init__()\n",
    "        # self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Load pretrained ESM2 model for proteins\n",
    "        self.modality1_model = AutoModel.from_pretrained(modality1_model_name)\n",
    "        self.modality1_tokenizer = AutoTokenizer.from_pretrained(modality1_model_name)\n",
    "        self.modality1_model.eval()\n",
    "        \n",
    "        # Load pretrained ChemLLM for SMILES (ligands)\n",
    "        self.modality2_model = AutoModel.from_pretrained(modality2_model_name)\n",
    "        self.modality2_tokenizer = AutoTokenizer.from_pretrained(modality2_model_name)\n",
    "        self.modality2_model.eval()\n",
    "\n",
    "        # Disable gradient computation for both base models\n",
    "        for param in self.modality1_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in self.modality2_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.modality1_embedding_dim = self.modality1_model.config.hidden_size\n",
    "        self.modality2_embedding_dim = self.modality2_model.config.hidden_size\n",
    "\n",
    "        # Projecting to the size of Modality1 model \n",
    "        self.project_to_common = nn.Linear(self.modality2_embedding_dim, self.modality1_embedding_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            CrossAttentionLayer(embed_dim=self.modality1_embedding_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.ffn_class_head = nn.Sequential(\n",
    "            nn.Linear(2 * self.modality1_embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(\n",
    "            self,\n",
    "            modality2_input_ids,\n",
    "            modality2_attention_mask,\n",
    "            modality1_input_ids,\n",
    "            modality1_attention_mask,\n",
    "        ):\n",
    "        # Protein embedding\n",
    "        # protein_inputs = self.modality1_tokenizer(protein_sequence, return_tensors=\"pt\")\n",
    "        modality1_inputs = {\n",
    "            \"input_ids\": modality1_input_ids,\n",
    "            \"attention_mask\": modality1_attention_mask\n",
    "        }\n",
    "        # perform in FP16 for lower memory usage (matmuls)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            with torch.no_grad():\n",
    "                modality1_outputs = self.modality1_model(**modality1_inputs)\n",
    "        special_tokens_mask_modality1 = (modality1_inputs['input_ids'] == self.modality1_tokenizer.cls_token_id)\\\n",
    "        | (modality1_inputs['input_ids'] == self.modality1_tokenizer.eos_token_id)\\\n",
    "        | (modality1_inputs['input_ids'] == self.modality1_tokenizer.pad_token_id)\n",
    "        modality1_embedding = modality1_outputs.last_hidden_state\n",
    "        \n",
    "        # SMILES embedding\n",
    "        modality2_inputs = {\n",
    "            \"input_ids\": modality2_input_ids,\n",
    "            \"attention_mask\": modality2_attention_mask\n",
    "        }\n",
    "        special_tokens_mask_modality2 = (modality2_inputs['input_ids'] == self.modality2_tokenizer.bos_token_id)\\\n",
    "        | (modality2_inputs['input_ids'] == self.modality2_tokenizer.eos_token_id)\\\n",
    "        | (modality2_inputs['input_ids'] == self.modality2_tokenizer.pad_token_id)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            with torch.no_grad():\n",
    "                modality2_outputs = self.modality2_model(**modality2_inputs)\n",
    "        modality2_embedding = modality2_outputs.last_hidden_state\n",
    "\n",
    "        # project embeddings to same dimension\n",
    "        modality2_embedding = self.project_to_common(modality2_embedding)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            modality1_embedding, modality2_embedding = layer(modality1_embedding, modality2_embedding, special_tokens_mask_modality1, special_tokens_mask_modality2)\n",
    "\n",
    "        # Perform mean pooling\n",
    "        modality2_embedding = (modality2_embedding * ~special_tokens_mask_modality2.unsqueeze(dim=-1)).mean(dim=1)\n",
    "        modality1_embedding = (modality1_embedding * ~special_tokens_mask_modality1.unsqueeze(dim=-1)).mean(dim=1)\n",
    "        # Combine embeddings\n",
    "        combined = torch.cat([modality1_embedding, modality2_embedding], dim=1)\n",
    "        logits = self.ffn_class_head(combined)\n",
    "        return logits\n",
    "\n",
    "\n",
    "esm_model_name = \"facebook/esm2_t33_650M_UR50D\"  # Replace with the correct ESM2 model name\n",
    "dna_model_name = \"InstaDeepAI/nucleotide-transformer-2.5b-multi-species\" # Replace with the correct ChemLLM model name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['HLA', 'peptide', 'Label', 'Length', 'Sequence', 'seqhash'],\n",
       "        num_rows: 26950\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['HLA', 'peptide', 'Label', 'Length', 'Sequence', 'seqhash'],\n",
       "        num_rows: 4314\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['HLA', 'peptide', 'Label', 'Length', 'Sequence', 'seqhash'],\n",
       "        num_rows: 4315\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data split \n",
    "from datasets import DatasetDict\n",
    "\n",
    "dataset_test = ds['test']\n",
    "dataset_test_val = dataset_test.train_test_split(test_size=0.5)\n",
    "\n",
    "dataset_dict = {\n",
    "    \"train\": ds['train'],\n",
    "    \"test\": dataset_test_val[\"train\"],\n",
    "    \"validation\": dataset_test_val['test']\n",
    "}\n",
    "dataset = DatasetDict(dataset_dict)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dna tokenizer is fast: False\n",
      "esm tokenizer is fast: False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca22199e6ecf49f89cfff57e21784296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26950 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a530e27e7e7e4763867842c540598f0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4314 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad804797a2ae48f4859c5708a5684717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4315 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4828951e074546bdbb0929e20ca5a189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26950 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "599015d7d8004a58a476fb4a290a10df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4314 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f3cca0d0f724d95a7b0bae194aef50f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4315 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['HLA', 'peptide', 'Label', 'Length', 'Sequence', 'seqhash', 'protein_input_ids', 'protein_attention_mask', 'dna_input_ids', 'dna_attention_mask'],\n",
       "        num_rows: 26950\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['HLA', 'peptide', 'Label', 'Length', 'Sequence', 'seqhash', 'protein_input_ids', 'protein_attention_mask', 'dna_input_ids', 'dna_attention_mask'],\n",
       "        num_rows: 4314\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['HLA', 'peptide', 'Label', 'Length', 'Sequence', 'seqhash', 'protein_input_ids', 'protein_attention_mask', 'dna_input_ids', 'dna_attention_mask'],\n",
       "        num_rows: 4315\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization of DNA and protein sequences\n",
    "dna_tokenizer = AutoTokenizer.from_pretrained(dna_model_name)\n",
    "esm_tokenizer = AutoTokenizer.from_pretrained(esm_model_name)\n",
    "print(f\"dna tokenizer is fast: {dna_tokenizer.is_fast}\")\n",
    "print(f\"esm tokenizer is fast: {esm_tokenizer.is_fast}\")\n",
    "\n",
    "def tokenize_dna(examples):\n",
    "    toks = dna_tokenizer(examples[\"Sequence\"], truncation=True)\n",
    "    return {\n",
    "        \"dna_input_ids\": toks[\"input_ids\"],\n",
    "        \"dna_attention_mask\": toks[\"attention_mask\"]\n",
    "    }\n",
    "\n",
    "def tokenize_proteins(examples):\n",
    "    toks =  esm_tokenizer(examples[\"peptide\"], truncation=True)\n",
    "    return {\n",
    "        \"protein_input_ids\": toks[\"input_ids\"],\n",
    "        \"protein_attention_mask\": toks[\"attention_mask\"]\n",
    "    }\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_proteins, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.map(tokenize_dna, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom data collator\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "class CustomDataCollator:\n",
    "    def __init__(self, dna_collator, esm_collator):\n",
    "            self.dna_collator = dna_collator\n",
    "            self.esm_collator = esm_collator\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        batch_dna = [{\"input_ids\": b[\"dna_input_ids\"], \"attention_mask\": b[\"dna_attention_mask\"], \"seqhash\": b[\"seqhash\"]} for b in batch]\n",
    "        # \"sequence\": b[\"Sequence\"]\n",
    "        batch_protein = [{\"input_ids\": b[\"protein_input_ids\"], \"attention_mask\": b[\"protein_attention_mask\"]} for b in batch]\n",
    "        # \"peptide\": b[\"peptide\"]\n",
    "\n",
    "        collated_dna = self.dna_collator(batch_dna)\n",
    "        collated_esm = self.esm_collator(batch_protein)\n",
    "\n",
    "        return {\n",
    "            \"seqhash\": collated_dna[\"seqhash\"],\n",
    "            \"dna_input_ids\": collated_dna[\"input_ids\"],\n",
    "            \"dna_attention_mask\": collated_dna[\"attention_mask\"],\n",
    "            \"protein_input_ids\": collated_esm[\"input_ids\"],\n",
    "            \"protein_attention_mask\": collated_esm[\"attention_mask\"],\n",
    "            \"label\": torch.tensor([x['Label'] for x in batch])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dna_collator = DataCollatorWithPadding(tokenizer=dna_tokenizer)\n",
    "esm_collator = DataCollatorWithPadding(tokenizer=esm_tokenizer)\n",
    "collator = CustomDataCollator(dna_collator=dna_collator, esm_collator=esm_collator)\n",
    "train_dataloader = DataLoader(tokenized_dataset[\"train\"], batch_size=BATCH_SIZE, collate_fn=collator)\n",
    "test_dataloader = DataLoader(tokenized_dataset[\"test\"], batch_size=BATCH_SIZE, collate_fn=collator)\n",
    "val_dataloader = DataLoader(tokenized_dataset[\"validation\"], batch_size=BATCH_SIZE, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['seqhash', 'dna_input_ids', 'dna_attention_mask', 'protein_input_ids', 'protein_attention_mask', 'label'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'seqhash': tensor([536476951, 741759667, 300766204, 382230261, 978239215, 866076692,\n",
       "         559776070,   3938397, 681186659,   3938397, 978239215,  26840829,\n",
       "         536476951, 924338967, 875213817, 924338967, 181395920, 936309151,\n",
       "         808768696, 536476951, 536476951, 773072430, 181395920, 928026744,\n",
       "         181395920, 864327756, 773072430, 866076692, 924338967, 690974567,\n",
       "         756305322, 713160395, 641458531, 626299351, 616152355, 229846056,\n",
       "         880764346, 832116058, 626299351, 690974567, 755341328, 928026744,\n",
       "         167834611, 369294224,  50307873,  32531134, 626299351, 755341328,\n",
       "          32301635, 341221573, 337884170, 766431600, 651416015, 626299351,\n",
       "         181395920, 866076692, 924338967, 429946180, 114916531, 536476951,\n",
       "         181395920, 766431600, 713160395, 924338967]),\n",
       " 'dna_input_ids': tensor([[   3,  499, 3471,  ..., 4100,    1,    1],\n",
       "         [   3,  499, 3467,  ..., 4101, 4103, 4100],\n",
       "         [   3,  499, 3471,  ..., 4100,    1,    1],\n",
       "         ...,\n",
       "         [   3,  510, 3467,  ...,    1,    1,    1],\n",
       "         [   3,  499, 3471,  ..., 4100,    1,    1],\n",
       "         [   3,  510, 3467,  ...,    1,    1,    1]]),\n",
       " 'dna_attention_mask': tensor([[1, 1, 1,  ..., 1, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'protein_input_ids': tensor([[ 0, 15,  8,  ...,  8,  2,  1],\n",
       "         [ 0, 21, 12,  ...,  1,  1,  1],\n",
       "         [ 0, 20,  9,  ...,  1,  1,  1],\n",
       "         ...,\n",
       "         [ 0,  6, 11,  ...,  1,  1,  1],\n",
       "         [ 0,  9,  6,  ...,  1,  1,  1],\n",
       "         [ 0,  4, 15,  ...,  1,  1,  1]]),\n",
       " 'protein_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'label': tensor([1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,\n",
       "         1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "         0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0])}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch = next(iter(train_dataloader))\n",
    "# print(batch.keys())\n",
    "# batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c6176b7e9e4c2b9a3f10ae52dd2811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at InstaDeepAI/nucleotide-transformer-2.5b-multi-species and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Training loop \n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "    \n",
    "def lr_lambda(step):\n",
    "        if step < WARMUP_STEPS:\n",
    "            # Linear warmup\n",
    "            return step / WARMUP_STEPS\n",
    "        else:\n",
    "            remaining_steps = total_steps - WARMUP_STEPS\n",
    "            decay_step = step - WARMUP_STEPS\n",
    "            return max(0.5 * LR, 1.0 - 0.5 * (decay_step / remaining_steps))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device {device}\")\n",
    "model = BindingAffinityModelWithMultiHeadCrossAttention(esm_model_name, dna_model_name).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=0)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "total_steps = EPOCHS * len(train_dataloader)\n",
    "\n",
    "\n",
    "def train_model(model, train_dataloader, val_dataloader):\n",
    "    step = 0\n",
    "    ACCUMULATION_STEPS = 2\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch: {epoch + 1}/{EPOCHS}\")\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_progress = tqdm(train_dataloader, desc=\"Training\")\n",
    "\n",
    "        for batch in train_progress:\n",
    "            dna_input_ids = batch[\"dna_input_ids\"].to(device)\n",
    "            dna_attention_mask = batch[\"dna_attention_mask\"].to(device)\n",
    "            protein_input_ids = batch[\"protein_input_ids\"].to(device)\n",
    "            protein_attention_mask = batch[\"protein_attention_mask\"].to(device)\n",
    "            targets = batch[\"label\"].unsqueeze(dim=-1).to(device)\n",
    "            preds = model(dna_input_ids, dna_attention_mask, protein_input_ids, protein_attention_mask)\n",
    "            loss = criterion(preds, targets.float())\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            step += 1\n",
    "            if step % ACCUMULATION_STEPS == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                wandb.log({\"train_loss\": loss.item()})\n",
    "                wandb.log({\"lr\": optimizer.param_groups[0][\"lr\"]})\n",
    "\n",
    "        train_loss /= len(train_dataloader)\n",
    "        print(f\"Epoch: {epoch} Train loss: {train_loss}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_progress = tqdm(val_dataloader, desc=\"Validation\")\n",
    "        with torch.no_grad():\n",
    "            for batch in val_progress:\n",
    "                dna_input_ids = batch[\"dna_input_ids\"].to(device)\n",
    "                dna_attention_mask = batch[\"dna_attention_mask\"].to(device)\n",
    "                protein_input_ids = batch[\"protein_input_ids\"].to(device)\n",
    "                protein_attention_mask = batch[\"protein_attention_mask\"].to(device)\n",
    "                targets = batch[\"label\"].unsqueeze(dim=-1).to(device)\n",
    "                preds = model(dna_input_ids, dna_attention_mask, protein_input_ids, protein_attention_mask)\n",
    "                loss = criterion(preds, targets.float())\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "        val_loss /= len(val_dataloader)\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch: {epoch} Val loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/422 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 422/422 [11:39<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train loss: 0.6098891632251829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:28<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Val loss: 0.5720075105919558\n",
      "Epoch: 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 422/422 [11:39<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.46977533816726286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:28<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Val loss: 0.5232838935711804\n",
      "Epoch: 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 422/422 [11:40<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Train loss: 0.437542302984197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:28<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Val loss: 0.5144918653018334\n",
      "Epoch: 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 422/422 [11:38<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Train loss: 0.404509483072995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:26<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Val loss: 0.41289742115665884\n",
      "Epoch: 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 422/422 [11:36<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Train loss: 0.3720863569898628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:27<00:00,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Val loss: 0.36971443672390547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 3,293,934,871\n",
      "Trainable parameters: 104,297,729\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n",
    "\n",
    "total, trainable = count_parameters(model)\n",
    "print(f\"Total parameters: {total:,}\")\n",
    "print(f\"Trainable parameters: {trainable:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test set: 100%|██████████| 68/68 [01:27<00:00,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8445\n",
      "Precision: 0.7971\n",
      "Recall: 0.9278\n",
      "F1-score: 0.8575\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.76      0.83      2138\n",
      "           1       0.80      0.93      0.86      2176\n",
      "\n",
      "    accuracy                           0.84      4314\n",
      "   macro avg       0.85      0.84      0.84      4314\n",
      "weighted avg       0.85      0.84      0.84      4314\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "\n",
    "\n",
    "# def evaluate_model(model, test_loader):\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "test_progress = tqdm(test_dataloader, desc=\"Test set\")\n",
    "with torch.no_grad():\n",
    "    for batch in test_progress:\n",
    "        ligand_input_ids = batch[\"dna_input_ids\"].to(device)\n",
    "        ligand_attention_mask = batch[\"dna_attention_mask\"].to(device)\n",
    "        protein_input_ids = batch[\"protein_input_ids\"].to(device)\n",
    "        protein_attention_mask = batch[\"protein_attention_mask\"].to(device)\n",
    "        targets = batch[\"label\"].unsqueeze(dim=-1).to(device)\n",
    "        preds = model(\n",
    "            ligand_input_ids,\n",
    "            ligand_attention_mask,\n",
    "            protein_input_ids,\n",
    "            protein_attention_mask,\n",
    "        )\n",
    "        # transform preds to 0 - 1 \n",
    "        # do sigmoid or sth\n",
    "        probs = torch.sigmoid(preds)\n",
    "        preds = (probs > 0.5).float()\n",
    "        all_targets.append(targets)\n",
    "        all_predictions.append(preds)\n",
    "\n",
    "all_predictions = torch.cat(all_predictions).cpu()\n",
    "all_targets = torch.cat(all_targets).cpu()\n",
    "\n",
    "accuracy = accuracy_score(all_targets, all_predictions)\n",
    "precision = precision_score(all_targets, all_predictions)\n",
    "recall = recall_score(all_targets, all_predictions)\n",
    "f1 = f1_score(all_targets, all_predictions)\n",
    "auc = roc_auc_score(all_targets, all_predictions)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(classification_report(all_targets, all_predictions))\n",
    "\n",
    "# evaluate_model(model, test_loader=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HLA': Value(dtype='string', id=None),\n",
       " 'peptide': Value(dtype='string', id=None),\n",
       " 'Label': Value(dtype='int64', id=None),\n",
       " 'Length': Value(dtype='int64', id=None),\n",
       " 'Sequence': Value(dtype='string', id=None),\n",
       " '__index_level_0__': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
