{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnikolamilicevic\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/nucl-protein/notebooks/wandb/run-20250317_211253-er6pnzil</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nikolamilicevic/nuclprot/runs/er6pnzil' target=\"_blank\">testing nuclprot CROSSATTN_FIN</a></strong> to <a href='https://wandb.ai/nikolamilicevic/nuclprot' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nikolamilicevic/nuclprot' target=\"_blank\">https://wandb.ai/nikolamilicevic/nuclprot</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nikolamilicevic/nuclprot/runs/er6pnzil' target=\"_blank\">https://wandb.ai/nikolamilicevic/nuclprot/runs/er6pnzil</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "WARMUP_STEPS = 6000\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 64\n",
    "LR = 2e-4\n",
    "CACHE_KEY = \"dna_key\"\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "wandb.init(\n",
    "    project=\"nuclprot\",\n",
    "    name=\"testing nuclprot CROSSATTN_FIN\",\n",
    "    config={\n",
    "        \"WARMUP_STEPS\": WARMUP_STEPS,\n",
    "        \"EPOCHS\": EPOCHS,\n",
    "        \"BATCH_SIZE\": BATCH_SIZE,\n",
    "        \"LR\": LR,\n",
    "    },\n",
    ")\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"vladak/anthem_hla_seq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['HLA', 'peptide', 'Label', 'Length', 'Sequence'],\n",
       "        num_rows: 10780\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['HLA', 'peptide', 'Label', 'Length', 'Sequence'],\n",
       "        num_rows: 3451\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subsample\n",
    "# from datasets import DatasetDict\n",
    "# ds = DatasetDict({\n",
    "#     split: ds[split].shuffle(seed=42).select(range(int(0.02 * len(ds[split]))))\n",
    "#     for split in ds\n",
    "# })\n",
    "# ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['HLA', 'peptide', 'Label', 'Length', 'Sequence'],\n",
       "        num_rows: 539019\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['HLA', 'peptide', 'Label', 'Length', 'Sequence'],\n",
       "        num_rows: 172580\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3de68df432fd4c27aad78796cf88cf65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/539019 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c61077a012214397a0968653a399290b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/172580 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['HLA', 'peptide', 'Label', 'Length', 'Sequence', 'dna_key'],\n",
       "        num_rows: 539019\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['HLA', 'peptide', 'Label', 'Length', 'Sequence', 'dna_key'],\n",
       "        num_rows: 172580\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hashlib\n",
    "def get_sequence_id(example):\n",
    "    example[CACHE_KEY] = int(hashlib.sha256(example['Sequence'].encode()).hexdigest(), 16) % (10**12)\n",
    "    return example\n",
    "\n",
    "ds = ds.map(get_sequence_id)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model registry as singleton.\n",
    "# Use this class to avoid creating the same model in multiple places\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "class ModelRegistry:\n",
    "    _models = {}\n",
    "    _tokenizers = {}\n",
    "    _instance = None\n",
    "    _device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    @classmethod\n",
    "    def get_model(cls, name):\n",
    "        if name not in cls._models:\n",
    "            model = AutoModel.from_pretrained(name).to(cls._device)\n",
    "            cls._models[name] = model\n",
    "        return cls._models[name]\n",
    "    \n",
    "    @classmethod\n",
    "    def get_tokenizer(cls, name):\n",
    "        if name not in cls._tokenizers:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "            cls._tokenizers[name] = tokenizer\n",
    "        return cls._tokenizers[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import DataCollatorWithPadding\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EmbeddingCache(torch.utils.data.Dataset):\n",
    "    \"\"\"This class will precompute embeddings for the data and cache\n",
    "    them for future reuse.\"\"\"\n",
    "    def __init__(self, data, key, input_ids_name, attention_mask_name, emb_model_name, device) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: Huggingface dataset that will be cached.\n",
    "            key: Column with unique values for each sample (e.g. sequence hash).\n",
    "            value: The value that we are computing embedding for and caching (input_ids + attn_mask).\n",
    "            emb_model: Model used for computing embeddings.\n",
    "            device: Device on which embeddings will reside.\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.key = key\n",
    "        self.emb_model_name = emb_model_name\n",
    "        self.emb_model = ModelRegistry.get_model(emb_model_name)\n",
    "        self.data = self._filter_duplicates(data, key)\n",
    "        self.cached_embeddings = self._cache_embeddings(input_ids_name, attention_mask_name)\n",
    "\n",
    "    def _filter_duplicates(self, data, key):\n",
    "        seen = set()\n",
    "        filtered_dataset =  data.filter(lambda example: not (example[key] in seen or seen.add(example[key])))\n",
    "        return filtered_dataset\n",
    "\n",
    "    def _cache_embeddings(self, input_ids_name, attention_mask_name):\n",
    "        embeddings_cache = {}\n",
    "        for sample in tqdm(self.data):\n",
    "            id = sample[self.key]\n",
    "            inputs = {\n",
    "                \"input_ids\": torch.tensor(sample[input_ids_name]).unsqueeze(0).to(self.device),\n",
    "                \"attention_mask\": torch.tensor(sample[attention_mask_name]).unsqueeze(0).to(self.device)\n",
    "            }\n",
    "            embeddings_cache[id] = self._compute_embedding(inputs)\n",
    "        return embeddings_cache\n",
    "\n",
    "    def _compute_embedding(self, inputs):\n",
    "        with torch.cuda.amp.autocast():\n",
    "            with torch.no_grad():\n",
    "                embedding = self.emb_model(**inputs).last_hidden_state.detach().cpu()\n",
    "        return embedding\n",
    "\n",
    "    def __getitem__(self, keys):\n",
    "        embeddings = []\n",
    "        for key in keys:\n",
    "            embeddings.append(self.cached_embeddings[key.item()])\n",
    "\n",
    "        # pad with 0s to the end\n",
    "        max_len = max(e.shape[1] for e in embeddings)\n",
    "        for i in range(len(embeddings)):\n",
    "            padding_size = max_len - embeddings[i].shape[1]\n",
    "            if padding_size > 0:\n",
    "                embeddings[i] = F.pad(embeddings[i], (0, 0, 0, padding_size))\n",
    "            embeddings[i] = embeddings[i].squeeze(0)\n",
    "        \n",
    "        stacked_embeddings = torch.stack(embeddings, dim=0)\n",
    "        return stacked_embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer, DataCollatorWithPadding\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CrossAttentionLayer(nn.Module):\n",
    "    def __init__(self, embed_dim=512, num_heads=8, dropout=0.1):\n",
    "        super(CrossAttentionLayer, self).__init__()\n",
    "        self.modality1_to_modality2_attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.modality2_to_modality1_attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        ffn_hidden_dim = embed_dim * 3\n",
    "        self.ffn_modality1 = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ffn_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ffn_hidden_dim, embed_dim),\n",
    "        )\n",
    "        self.ffn_modality2 = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ffn_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ffn_hidden_dim, embed_dim),\n",
    "        )\n",
    "\n",
    "        self.modality1_norm = nn.LayerNorm(embed_dim)\n",
    "        self.modality2_norm = nn.LayerNorm(embed_dim)\n",
    "        self.ffn_modality1_norm = nn.LayerNorm(embed_dim)\n",
    "        self.ffn_modality2_norm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, modality1_embedding, modality2_embedding, key_pad_mask_modality1, key_pad_mask_modality2):\n",
    "        # Modality1 attending to Modality2\n",
    "        attended_modality1, _ = self.modality1_to_modality2_attention(\n",
    "            query=modality1_embedding, \n",
    "            key=modality2_embedding,\n",
    "            value=modality2_embedding,\n",
    "            key_padding_mask=key_pad_mask_modality2\n",
    "        )\n",
    "        attended_modality1 = self.modality1_norm(modality1_embedding + attended_modality1)\n",
    "        x_modality1 = self.ffn_modality1(attended_modality1)\n",
    "        x_modality1 = self.ffn_modality1_norm(attended_modality1 + self.dropout(x_modality1))\n",
    "\n",
    "        # Modality2 attending to Modality1\n",
    "        attended_modality2, _ = self.modality2_to_modality1_attention(\n",
    "            query=modality2_embedding, \n",
    "            key=modality1_embedding, \n",
    "            value=modality1_embedding,\n",
    "            key_padding_mask=key_pad_mask_modality1\n",
    "        )\n",
    "        attended_modality2 = self.modality2_norm(modality2_embedding + attended_modality2)\n",
    "        x_modality2 = self.ffn_modality2(attended_modality2)\n",
    "        x_modality2 = self.ffn_modality2_norm(attended_modality2 + self.dropout(x_modality2))\n",
    "\n",
    "        return x_modality1, x_modality2\n",
    "\n",
    "class BindingAffinityModelWithMultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self, modality1_model_name, modality2_model_name, num_layers=3, hidden_dim=1024, modality1_cache=None, modality2_cache=None):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Load pretrained ESM2 model for proteins\n",
    "        self.modality1_model = ModelRegistry.get_model(modality1_model_name)\n",
    "        self.modality1_tokenizer = ModelRegistry.get_tokenizer(modality1_model_name)\n",
    "        self.modality1_model.eval()\n",
    "        \n",
    "        # Load pretrained ChemLLM for SMILES (ligands)\n",
    "        self.modality2_model = ModelRegistry.get_model(modality2_model_name)\n",
    "        self.modality2_tokenizer = ModelRegistry.get_tokenizer(modality2_model_name)\n",
    "        self.modality2_model.eval()\n",
    "\n",
    "        self.modality1_cache = modality1_cache\n",
    "        self.modality2_cache = modality2_cache\n",
    "\n",
    "        # Disable gradient computation for both base models\n",
    "        for param in self.modality1_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in self.modality2_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.modality1_embedding_dim = self.modality1_model.config.hidden_size\n",
    "        self.modality2_embedding_dim = self.modality2_model.config.hidden_size\n",
    "\n",
    "        # Projecting to the size of Modality1 model \n",
    "        self.project_to_common = nn.Linear(self.modality2_embedding_dim, self.modality1_embedding_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            CrossAttentionLayer(embed_dim=self.modality1_embedding_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.ffn_class_head = nn.Sequential(\n",
    "            nn.Linear(2 * self.modality1_embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(\n",
    "            self,\n",
    "            modality1_input_ids,\n",
    "            modality1_attention_mask,\n",
    "            modality2_input_ids,\n",
    "            modality2_attention_mask,\n",
    "            modality1_cache_keys=None,\n",
    "            modality2_cache_keys=None,\n",
    "        ):\n",
    "        # Modality 1\n",
    "        modality1_inputs = {\n",
    "            \"input_ids\": modality1_input_ids,\n",
    "            \"attention_mask\": modality1_attention_mask\n",
    "        }\n",
    "        \n",
    "        if self.modality1_cache:\n",
    "            modality1_embedding = self.modality1_cache[modality1_cache_keys]\n",
    "            modality1_embedding = modality1_embedding.to(self.device)\n",
    "        else:\n",
    "            # perform in FP16 for lower memory usage (matmuls)\n",
    "            with torch.cuda.amp.autocast():\n",
    "                with torch.no_grad():\n",
    "                    modality1_outputs = self.modality1_model(**modality1_inputs)\n",
    "                    modality1_embedding = modality1_outputs.last_hidden_state\n",
    "        \n",
    "        special_tokens_mask_modality1 = (modality1_inputs['input_ids'] == self.modality1_tokenizer.cls_token_id)\\\n",
    "        | (modality1_inputs['input_ids'] == self.modality1_tokenizer.eos_token_id)\\\n",
    "        | (modality1_inputs['input_ids'] == self.modality1_tokenizer.pad_token_id)\n",
    "        \n",
    "        # Modality 2\n",
    "        modality2_inputs = {\n",
    "            \"input_ids\": modality2_input_ids,\n",
    "            \"attention_mask\": modality2_attention_mask\n",
    "        }\n",
    "    \n",
    "        if self.modality2_cache:\n",
    "            modality2_embedding = self.modality2_cache[modality2_cache_keys]\n",
    "            modality2_embedding = modality2_embedding.to(self.device)\n",
    "        else:\n",
    "            # perform in FP16 for lower memory usage (matmuls)\n",
    "            with torch.cuda.amp.autocast():\n",
    "                with torch.no_grad():\n",
    "                    modality2_outputs = self.modality2_model(**modality2_inputs)\n",
    "                    modality2_embedding = modality2_outputs.last_hidden_state\n",
    "\n",
    "        special_tokens_mask_modality2 = (modality2_inputs['input_ids'] == self.modality2_tokenizer.cls_token_id)\\\n",
    "        | (modality2_inputs['input_ids'] == self.modality2_tokenizer.eos_token_id)\\\n",
    "        | (modality2_inputs['input_ids'] == self.modality2_tokenizer.pad_token_id)\n",
    "\n",
    "        # project embeddings to same dimension\n",
    "        modality2_embedding = self.project_to_common(modality2_embedding)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            modality1_embedding, modality2_embedding = layer(modality1_embedding, modality2_embedding, special_tokens_mask_modality1, special_tokens_mask_modality2)\n",
    "\n",
    "        # Perform mean pooling\n",
    "        modality2_embedding = (modality2_embedding * ~special_tokens_mask_modality2.unsqueeze(dim=-1)).mean(dim=1)\n",
    "        modality1_embedding = (modality1_embedding * ~special_tokens_mask_modality1.unsqueeze(dim=-1)).mean(dim=1)\n",
    "        # Combine embeddings\n",
    "        combined = torch.cat([modality1_embedding, modality2_embedding], dim=1)\n",
    "        logits = self.ffn_class_head(combined)\n",
    "        return logits\n",
    "\n",
    "\n",
    "esm_model_name = \"facebook/esm2_t33_650M_UR50D\"  # Replace with the correct ESM2 model name\n",
    "dna_model_name = \"InstaDeepAI/nucleotide-transformer-2.5b-multi-species\" # Replace with the correct ChemLLM model name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['HLA', 'peptide', 'Label', 'Length', 'Sequence', 'dna_key'],\n",
       "        num_rows: 10780\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['HLA', 'peptide', 'Label', 'Length', 'Sequence', 'dna_key'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['HLA', 'peptide', 'Label', 'Length', 'Sequence', 'dna_key'],\n",
       "        num_rows: 1726\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data split \n",
    "from datasets import DatasetDict\n",
    "\n",
    "dataset_test = ds['test']\n",
    "dataset_test_val = dataset_test.train_test_split(test_size=0.5)\n",
    "\n",
    "dataset_dict = {\n",
    "    \"train\": ds['train'],\n",
    "    \"test\": dataset_test_val[\"train\"],\n",
    "    \"validation\": dataset_test_val['test']\n",
    "}\n",
    "dataset = DatasetDict(dataset_dict)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dna tokenizer is fast: False\n",
      "esm tokenizer is fast: False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cd4a3749e9547feb17fcb144e779372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff9732fa8afb446280568f7da3e5b99d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1726 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cbf09d6598b478996bf8ed77d805489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e93cbd211bff4da19bee46ac019329e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1726 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['HLA', 'peptide', 'Label', 'Length', 'Sequence', 'dna_key', 'protein_input_ids', 'protein_attention_mask', 'dna_input_ids', 'dna_attention_mask'],\n",
       "        num_rows: 10780\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['HLA', 'peptide', 'Label', 'Length', 'Sequence', 'dna_key', 'protein_input_ids', 'protein_attention_mask', 'dna_input_ids', 'dna_attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['HLA', 'peptide', 'Label', 'Length', 'Sequence', 'dna_key', 'protein_input_ids', 'protein_attention_mask', 'dna_input_ids', 'dna_attention_mask'],\n",
       "        num_rows: 1726\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization of DNA and protein sequences\n",
    "dna_tokenizer = AutoTokenizer.from_pretrained(dna_model_name)\n",
    "esm_tokenizer = AutoTokenizer.from_pretrained(esm_model_name)\n",
    "print(f\"dna tokenizer is fast: {dna_tokenizer.is_fast}\")\n",
    "print(f\"esm tokenizer is fast: {esm_tokenizer.is_fast}\")\n",
    "\n",
    "def tokenize_dna(examples):\n",
    "    toks = dna_tokenizer(examples[\"Sequence\"])\n",
    "    return {\n",
    "        \"dna_input_ids\": toks[\"input_ids\"],\n",
    "        \"dna_attention_mask\": toks[\"attention_mask\"]\n",
    "    }\n",
    "\n",
    "def tokenize_proteins(examples):\n",
    "    toks =  esm_tokenizer(examples[\"peptide\"], truncation=True)\n",
    "    return {\n",
    "        \"protein_input_ids\": toks[\"input_ids\"],\n",
    "        \"protein_attention_mask\": toks[\"attention_mask\"]\n",
    "    }\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_proteins, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.map(tokenize_dna, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom data collator\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "class CustomDataCollator:\n",
    "    def __init__(self, dna_collator, esm_collator):\n",
    "            self.dna_collator = dna_collator\n",
    "            self.esm_collator = esm_collator\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        batch_dna = [{\"input_ids\": b[\"dna_input_ids\"], \"attention_mask\": b[\"dna_attention_mask\"], CACHE_KEY: b[CACHE_KEY]} for b in batch]\n",
    "        # \"sequence\": b[\"Sequence\"]\n",
    "        batch_protein = [{\"input_ids\": b[\"protein_input_ids\"], \"attention_mask\": b[\"protein_attention_mask\"]} for b in batch]\n",
    "        # \"peptide\": b[\"peptide\"]\n",
    "\n",
    "        collated_dna = self.dna_collator(batch_dna)\n",
    "        collated_esm = self.esm_collator(batch_protein)\n",
    "\n",
    "        return {\n",
    "            CACHE_KEY: collated_dna[CACHE_KEY],\n",
    "            \"dna_input_ids\": collated_dna[\"input_ids\"],\n",
    "            \"dna_attention_mask\": collated_dna[\"attention_mask\"],\n",
    "            \"protein_input_ids\": collated_esm[\"input_ids\"],\n",
    "            \"protein_attention_mask\": collated_esm[\"attention_mask\"],\n",
    "            \"label\": torch.tensor([x['Label'] for x in batch])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dna_collator = DataCollatorWithPadding(tokenizer=dna_tokenizer)\n",
    "esm_collator = DataCollatorWithPadding(tokenizer=esm_tokenizer)\n",
    "collator = CustomDataCollator(dna_collator=dna_collator, esm_collator=esm_collator)\n",
    "train_dataloader = DataLoader(tokenized_dataset[\"train\"], batch_size=BATCH_SIZE, collate_fn=collator)\n",
    "test_dataloader = DataLoader(tokenized_dataset[\"test\"], batch_size=BATCH_SIZE, collate_fn=collator)\n",
    "val_dataloader = DataLoader(tokenized_dataset[\"validation\"], batch_size=BATCH_SIZE, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = torch.tensor(tokenized_dataset[\"train\"][CACHE_KEY][:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:08<00:00, 12.39it/s]\n",
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Training loop \n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "    \n",
    "def lr_lambda(step):\n",
    "        if step < WARMUP_STEPS:\n",
    "            # Linear warmup\n",
    "            return step / WARMUP_STEPS\n",
    "        else:\n",
    "            remaining_steps = total_steps - WARMUP_STEPS\n",
    "            decay_step = step - WARMUP_STEPS\n",
    "            return max(0.5 * LR, 1.0 - 0.5 * (decay_step / remaining_steps))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device {device}\")\n",
    "dna_cache = EmbeddingCache(data=tokenized_dataset[\"train\"], key=CACHE_KEY, input_ids_name=\"dna_input_ids\", attention_mask_name=\"dna_attention_mask\", emb_model_name=dna_model_name, device=device)\n",
    "model = BindingAffinityModelWithMultiHeadCrossAttention(modality1_model_name=esm_model_name, modality2_model_name=dna_model_name, modality2_cache=dna_cache).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=0)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "total_steps = EPOCHS * len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, val_dataloader):\n",
    "    step = 0\n",
    "    ACCUMULATION_STEPS = 2\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch: {epoch + 1}/{EPOCHS}\")\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_progress = tqdm(train_dataloader, desc=\"Training\")\n",
    "\n",
    "        for batch in train_progress:\n",
    "            dna_input_ids = batch[\"dna_input_ids\"].to(device)\n",
    "            dna_attention_mask = batch[\"dna_attention_mask\"].to(device)\n",
    "            protein_input_ids = batch[\"protein_input_ids\"].to(device)\n",
    "            protein_attention_mask = batch[\"protein_attention_mask\"].to(device)\n",
    "            dna_cache_keys=batch[CACHE_KEY]\n",
    "            targets = batch[\"label\"].unsqueeze(dim=-1).to(device)\n",
    "            preds = model(\n",
    "                modality1_input_ids=protein_input_ids, \n",
    "                modality1_attention_mask=protein_attention_mask, \n",
    "                modality2_input_ids=dna_input_ids, \n",
    "                modality2_attention_mask=dna_attention_mask,\n",
    "                modality2_cache_keys=dna_cache_keys\n",
    "                )\n",
    "            loss = criterion(preds, targets.float())\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            step += 1\n",
    "            if step % ACCUMULATION_STEPS == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                wandb.log({\"train_loss\": loss.item()})\n",
    "                wandb.log({\"lr\": optimizer.param_groups[0][\"lr\"]})\n",
    "\n",
    "        train_loss /= len(train_dataloader)\n",
    "        print(f\"Epoch: {epoch} Train loss: {train_loss}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_progress = tqdm(val_dataloader, desc=\"Validation\")\n",
    "        with torch.no_grad():\n",
    "            for batch in val_progress:\n",
    "                dna_input_ids = batch[\"dna_input_ids\"].to(device)\n",
    "                dna_attention_mask = batch[\"dna_attention_mask\"].to(device)\n",
    "                protein_input_ids = batch[\"protein_input_ids\"].to(device)\n",
    "                protein_attention_mask = batch[\"protein_attention_mask\"].to(device)\n",
    "                dna_cache_keys=batch[CACHE_KEY]\n",
    "                targets = batch[\"label\"].unsqueeze(dim=-1).to(device)\n",
    "                preds = model(\n",
    "                    modality1_input_ids=protein_input_ids, \n",
    "                    modality1_attention_mask=protein_attention_mask, \n",
    "                    modality2_input_ids=dna_input_ids, \n",
    "                    modality2_attention_mask=dna_attention_mask,\n",
    "                    modality2_cache_keys=dna_cache_keys\n",
    "                )\n",
    "                loss = criterion(preds, targets.float())\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "        val_loss /= len(val_dataloader)\n",
    "        print(f\"Epoch: {epoch} Val loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/169 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▏      | 54/169 [19:38<01:16,  1.51it/s]   "
     ]
    }
   ],
   "source": [
    "train_model(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 3,293,934,871\n",
      "Trainable parameters: 104,297,729\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n",
    "\n",
    "total, trainable = count_parameters(model)\n",
    "print(f\"Total parameters: {total:,}\")\n",
    "print(f\"Trainable parameters: {trainable:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test set: 100%|██████████| 68/68 [01:27<00:00,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8445\n",
      "Precision: 0.7971\n",
      "Recall: 0.9278\n",
      "F1-score: 0.8575\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.76      0.83      2138\n",
      "           1       0.80      0.93      0.86      2176\n",
      "\n",
      "    accuracy                           0.84      4314\n",
      "   macro avg       0.85      0.84      0.84      4314\n",
      "weighted avg       0.85      0.84      0.84      4314\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "\n",
    "\n",
    "# def evaluate_model(model, test_loader):\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "test_progress = tqdm(test_dataloader, desc=\"Test set\")\n",
    "with torch.no_grad():\n",
    "    for batch in test_progress:\n",
    "        ligand_input_ids = batch[\"dna_input_ids\"].to(device)\n",
    "        ligand_attention_mask = batch[\"dna_attention_mask\"].to(device)\n",
    "        protein_input_ids = batch[\"protein_input_ids\"].to(device)\n",
    "        protein_attention_mask = batch[\"protein_attention_mask\"].to(device)\n",
    "        targets = batch[\"label\"].unsqueeze(dim=-1).to(device)\n",
    "        preds = model(\n",
    "            ligand_input_ids,\n",
    "            ligand_attention_mask,\n",
    "            protein_input_ids,\n",
    "            protein_attention_mask,\n",
    "        )\n",
    "        # transform preds to 0 - 1 \n",
    "        # do sigmoid or sth\n",
    "        probs = torch.sigmoid(preds)\n",
    "        preds = (probs > 0.5).float()\n",
    "        all_targets.append(targets)\n",
    "        all_predictions.append(preds)\n",
    "\n",
    "all_predictions = torch.cat(all_predictions).cpu()\n",
    "all_targets = torch.cat(all_targets).cpu()\n",
    "\n",
    "accuracy = accuracy_score(all_targets, all_predictions)\n",
    "precision = precision_score(all_targets, all_predictions)\n",
    "recall = recall_score(all_targets, all_predictions)\n",
    "f1 = f1_score(all_targets, all_predictions)\n",
    "auc = roc_auc_score(all_targets, all_predictions)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(classification_report(all_targets, all_predictions))\n",
    "\n",
    "# evaluate_model(model, test_loader=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HLA': Value(dtype='string', id=None),\n",
       " 'peptide': Value(dtype='string', id=None),\n",
       " 'Label': Value(dtype='int64', id=None),\n",
       " 'Length': Value(dtype='int64', id=None),\n",
       " 'Sequence': Value(dtype='string', id=None),\n",
       " '__index_level_0__': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
